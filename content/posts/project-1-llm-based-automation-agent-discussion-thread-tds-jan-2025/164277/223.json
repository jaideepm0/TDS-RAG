{"id": 594439, "name": "Arya Agrahari ", "username": "22f3002771", "avatar_template": "/user_avatar/discourse.onlinedegree.iitm.ac.in/22f3002771/{size}/66809_2.png", "created_at": "2025-02-13T11:09:48.608Z", "cooked": "<p>i am getting unauthorized error in A9 again and again, i have pasted my code if someone can help please look into this.</p>\n<pre><code class=\"lang-auto\"># /// script\n# requires-python = \"&gt;=3.11\"\n# dependencies = [\n#   \"numpy\",\n#   \"httpx\",\n#   \"fastapi\",\n# ]\n# ///\n\n\nimport httpx\nimport numpy as np\nimport datetime\nimport os\n\nfrom fastapi import HTTPException\n\n\nnow = datetime.datetime.now()\n\nOPENAI_API_KEY = os.environ[\"AIPROXY_TOKEN\"]\nOPENAI_API_URL = \"http://aiproxy.sanand.workers.dev/openai/v1/embeddings\"\n\n\n# async def get_similarity_from_embeddings(emb1: list[float], emb2: list[float]) -&gt; float:\ndef get_similarity_from_embeddings(emb1: list[float], emb2: list[float]) -&gt; float:\n    # \"\"\"Calculate cosine similarity between two texts.\"\"\"\n    # emb1 = await embed(text1)\n    # emb2 = await embed(text2)\n    return float(np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2)))\n\n\n# async def embed_list(text_list: list[str]) -&gt; list[float]:\nasync def embed_list(text_list: list[str]) -&gt; list[float]:\n    OPENAI_API_KEY = os.environ[\"AIPROXY_TOKEN\"]\n    OPENAI_API_URL = \"http://aiproxy.sanand.workers.dev/openai/v1/embeddings\"\n    \"\"\"Get embedding vector for text using OpenAI's API.\"\"\"\n    try:\n        async with httpx.AsyncClient() as client:\n            # with httpx.AsyncClient() as client:\n            response = await client.post(\n                # response = httpx.post(\n                OPENAI_API_URL,\n                headers={\"Authorization\": f\"Bearer {OPENAI_API_KEY}\"},\n                \n                json={\"model\": \"text-embedding-3-small\", \"input\": text_list},\n            )\n        # print(f'{response.json()[\"data\"][0][\"embedding\"]}')\n        emb_list = [emb[\"embedding\"] for emb in response.json()[\"data\"]]\n        print(f\"Number of embeddings returned = {len(emb_list)}\")\n        return emb_list\n\n    except KeyError as e:\n        print(f\"INSIDE EMBED_LIST IN A9. KeyError occurred while querying GPT: {e}\")\n        raise HTTPException(status_code=400, detail=str(e))\n\n    except Exception as e:\n        print(f\"INSIDE EMBED_LIST IN A9. General Error while querying gpt: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\ndef most_similar(embeddings):\n    # Extract the phrases and their corresponding embeddings\n    phrases = list(embeddings.keys())\n    emb_values = list(embeddings.values())\n\n    # Initialize variables to track the maximum similarity\n    max_similarity = -1  # Start with the smallest possible similarity value\n    most_similar_pair = None\n\n    # Compute cosine similarity between each pair of embeddings\n    for i in range(len(emb_values)):\n        for j in range(i + 1, len(emb_values)):  # Avoid repeating pairs\n            similarity = get_similarity_from_embeddings(emb_values[i], emb_values[j])\n            if similarity &gt; max_similarity:\n                max_similarity = similarity\n                most_similar_pair = (phrases[i], phrases[j])\n\n    return most_similar_pair\n\n\n# async def get_similar_comments(input_file_path: str, output_file_path: str):\nasync def get_similar_comments(input_file_path: str, output_file_path: str):\n    print(f\"Reading the input file: {input_file_path}\")\n    with open(input_file_path, \"r\") as file:\n        comments = file.readlines()\n\n    print(f\"Embedding the comments\")\n    # embeddings = await embed_list(comments)\n    embeddings = await embed_list(comments)\n    embed_dict = dict(zip(comments, embeddings))\n    most_similar_pair = most_similar(embed_dict)\n    print(f\"Most similar comments: {most_similar_pair}\")\n\n    with open(output_file_path, \"w\") as file:\n        for comment in most_similar_pair:\n            file.write(f\"{comment.strip()}\\n\")\n        # file.write(f\"Most similar comments: {most_similar_pair}\")\n\n\nif __name__ == \"__main__\":\n    # import asyncio\n\n    input_file_path = \"/data/comments.txt\"\n    output_file_path = \"/data/similar_comments.txt\"\n    # asyncio.run(get_similar_comments(input_file_path, output_file_path))\n    get_similar_comments(input_file_path, output_file_path)\n</code></pre>", "post_number": 223, "post_type": 1, "posts_count": 614, "updated_at": "2025-02-13T11:09:48.608Z", "reply_count": 2, "reply_to_post_number": null, "quote_count": 0, "incoming_link_count": 2, "reads": 133, "readers_count": 132, "score": 46.4, "yours": false, "topic_id": 164277, "topic_slug": "project-1-llm-based-automation-agent-discussion-thread-tds-jan-2025", "display_username": "Arya Agrahari ", "primary_group_name": "ds-students", "flair_name": "ds-students", "flair_url": null, "flair_bg_color": "", "flair_color": "", "flair_group_id": 294, "badges_granted": [], "version": 1, "can_edit": false, "can_delete": false, "can_recover": false, "can_see_hidden_post": false, "can_wiki": false, "read": true, "user_title": null, "bookmarked": false, "actions_summary": [{"id": 2, "can_act": true}, {"id": 6, "can_act": true}, {"id": 3, "can_act": true}, {"id": 4, "can_act": true}, {"id": 8, "can_act": true}, {"id": 10, "can_act": true}, {"id": 7, "can_act": true}], "moderator": false, "admin": false, "staff": false, "user_id": 17017, "hidden": false, "trust_level": 1, "deleted_at": null, "user_deleted": false, "edit_reason": null, "can_view_edit_history": true, "wiki": false, "post_url": "/t/project-1-llm-based-automation-agent-discussion-thread-tds-jan-2025/164277/223", "user_cakedate": "2023-03-20", "reactions": [], "current_user_reaction": null, "reaction_users_count": 0, "current_user_used_main_reaction": false, "can_accept_answer": false, "can_unaccept_answer": false, "accepted_answer": false, "topic_accepted_answer": null}