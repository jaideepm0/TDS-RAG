{"id": 615386, "name": "Shreyan Chaubey", "username": "thinkmachine", "avatar_template": "/user_avatar/discourse.onlinedegree.iitm.ac.in/thinkmachine/{size}/131574_2.png", "created_at": "2025-04-04T22:16:43.757Z", "cooked": "<p>Thanks for the clarification regarding the evaluation, <a class=\"mention\" href=\"/u/carlton\">@carlton</a>. It\u2019s a relief to know that my original submission was successfully evaluated. Had I known that the stricter evaluation script would pull the image matching the digest from the time of submission (which had been overwritten by April 1), I would\u2019ve used a separate tag to avoid the issue altogether.</p>\n<p>Regarding your point on gold plating \u2014 I completely understand and have come to appreciate the importance of building to spec from personal experience, especially in production or client-facing settings where fixed targets, maintainability, and minimal scope creep are key. That said, with TDS projects, my goal was purely exploratory \u2014 to explore the boundaries of what\u2019s possible <strong>within the scope of the problem statement</strong>.</p>\n<p>What began as just another (pun intended) <em>tedious</em> assignment slowly evolved into a hobbyist research project on LLM agents. <img src=\"https://emoji.discourse-cdn.com/google/grinning_face_with_smiling_eyes.png?v=14\" title=\":grinning_face_with_smiling_eyes:\" class=\"emoji\" alt=\":grinning_face_with_smiling_eyes:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p><em>(\u2026caution: long post ahead <img src=\"https://emoji.discourse-cdn.com/google/sweat_smile.png?v=14\" title=\":sweat_smile:\" class=\"emoji\" alt=\":sweat_smile:\" loading=\"lazy\" width=\"20\" height=\"20\">)</em></p>\n<p>I noticed that <strong>test cases in Project 1 and 2 were highly specific and often overlapping on Python &amp; Shell use</strong>. While it would\u2019ve been easy to hardcode 50+ Python functions to pass these cases (which, frankly, many of us were doing), it is non-scalable at best. I quickly realized that stochastic parrots + hardcoded functions were a recipe for disaster, especially considering the <strong>inherent randomness in LLM-generated payloads</strong>. No two payloads are exactly alike \u2014 even minor differences, like an absolute vs relative file path, or some hidden edge case could cause a hardcoded solution to fail unpredictably. There\u2019s also really no way to predict an edge case caused by an LLM.</p>\n<p>Some might suggest using <code>temperature=0</code> to get more deterministic LLM behavior \u2014 and while true to an extent, it does little to encourage exploration, especially in tasks that require self-correction based on environmental feedback. Prompt engineering too wouldn\u2019t be helpful here as 4o-mini isn\u2019t all that great at 0-shot instruction following, especially when the system prompt is already saturated with 50+ fine-grained instructions. There\u2019s only so much stuff it can pay attention to.</p>\n<p><strong>Hardcoded tool agents also aren\u2019t really \u201cagents\u201d in my view\u2014 they\u2019re more like passive AI powered regex matchers</strong>: merely mapping inputs to functions by inferring from context window. That puts all the burden of answering on the hardcoded functions, leaving the agent itself uninvolved in the solutioning process. If they break, the agent will never try to \u2018fix\u2019 them and keep calling them like a broken record.</p>\n<p>At the core of it, it\u2019s all about <strong>how much flexibility vs rigidity</strong> we give to the agent. Fully rigid solutions (e.g. hardcoding) overfit and break easily; fully flexible ones (e.g. pure LLM based) hallucinate or drift off-target. The sweet spot lies somewhere in between \u2014 The right solution would naturally balance the lesser of two evils in an ideal ratio.</p>\n<p>Since most LLMs already excel at code generation and structured solutioning, the most effective strategy that I figured out for the agent was to,</p>\n<ul>\n<li>Reason about the task, understand intent,</li>\n<li>Reflect, whether this problem is solvable using available tools without human intervention and design structured workflows, in whatever order appropriate (i.e. <em>design</em> a DAG, where each node can be a python step or a shell step or something else)</li>\n<li>Execute those workflows (<em>walk</em> the DAG) observing the feedback at each step and reiterating if needed.</li>\n<li>Observe the final result, and repeat if needed.</li>\n</ul>\n<p>Interestingly, a similar framework was suggested in <a href=\"https://arxiv.org/abs/2210.03629\" rel=\"noopener nofollow ugc\">this ICLR 2022 paper</a>. That was all the validation I needed to know I was stepping in the right direction.</p>\n<p>To make environment interaction possible, the agent didn\u2019t need dozens of narrow tools \u2014 just a small, well-defined set of <strong>general-purpose tools</strong>:</p>\n<ul>\n<li>A REPL executor (for quick calcs)</li>\n<li>A Python script runner</li>\n<li>A Shell executor</li>\n</ul>\n<p>With just these, it could handle most tasks flexibly and naturally \u2014 avoiding overengineering while still enabling powerful behaviors. Potentially allowing for full fledged Computer-Use via shell and so much more.</p>\n<p>As for the fact that it ended up being capable of things beyond the scope of Project 1 (like training &amp; tuning ML models autonomously, reporting results etc.) \u2014 that was <strong>emergent behavior</strong>, not deliberate gold plating. It was a pleasant surprise even to me. I\u2019ve yet to discover more of such interesting hidden use cases. While some might naturally call it scope creeping (and yes that is true, given that we had a deadline, and a play-pretend client-business relationship with the course team), I saw it as an opportunity for exploration and research. <em>Frankly, I AM personally very keen about researching stuff!</em></p>\n<p>I am actually very thankful to the TDS course team &amp; <a class=\"mention\" href=\"/u/s.anand\">@s.anand</a> for devising such a thoughtful project that sparked some interesting ideas that I can tinker with. <strong>Food for thought! Really!</strong></p>\n<p>As for my next project, I now have a fair idea of what I\u2019ll be experimenting with\u2014 modalities.</p>\n<p>PS: I\u2019m not claiming it\u2019s perfect or production-ready, or it should score a perfect 22/20, but it aligned well with what I believe was the spirit of these projects: <strong>thoughtful use of LLMs in agent design</strong>. At this point, I\u2019m less concerned about the marks, I\u2019m actually enjoying the thought joyride. <img src=\"https://emoji.discourse-cdn.com/google/grinning_face_with_smiling_eyes.png?v=14\" title=\":grinning_face_with_smiling_eyes:\" class=\"emoji\" alt=\":grinning_face_with_smiling_eyes:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<hr>\n<p><strong>TL;DR</strong><br>\nMy approach doesn\u2019t rely on regex or hardcoded mappings. Instead, it passes user input directly to an LLM, which then plans and executes workflows using general tools inside a containerized environment. It also learns from feedback and iterates \u2014 much like a human. The fact that it can do more than just the minimum spec is a byproduct of that framework. I\u2019ve only just wired the pieces together.</p>\n<p>Kind regards</p>", "post_number": 289, "post_type": 1, "posts_count": 467, "updated_at": "2025-04-04T22:16:43.757Z", "reply_count": 0, "reply_to_post_number": 285, "quote_count": 0, "incoming_link_count": 0, "reads": 82, "readers_count": 81, "score": 31.0, "yours": false, "topic_id": 171141, "topic_slug": "tds-official-project1-discrepencies", "display_username": "Shreyan Chaubey", "primary_group_name": "ds-students", "flair_name": "ds-students", "flair_url": null, "flair_bg_color": "", "flair_color": "", "flair_group_id": 294, "badges_granted": [], "version": 1, "can_edit": false, "can_delete": false, "can_recover": false, "can_see_hidden_post": false, "can_wiki": false, "link_counts": [{"url": "https://arxiv.org/abs/2210.03629", "internal": false, "reflection": false, "title": "[2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models", "clicks": 0}], "read": false, "user_title": "", "reply_to_user": {"id": 16094, "username": "carlton", "name": "Carlton D'Silva", "avatar_template": "/user_avatar/discourse.onlinedegree.iitm.ac.in/carlton/{size}/56317_2.png"}, "bookmarked": false, "actions_summary": [{"id": 2, "count": 1, "can_act": true}, {"id": 6, "can_act": true}, {"id": 3, "can_act": true}, {"id": 4, "can_act": true}, {"id": 8, "can_act": true}, {"id": 10, "can_act": true}, {"id": 7, "can_act": true}], "moderator": false, "admin": false, "staff": false, "user_id": 16332, "hidden": false, "trust_level": 1, "deleted_at": null, "user_deleted": false, "edit_reason": null, "can_view_edit_history": true, "wiki": false, "post_url": "/t/tds-official-project1-discrepencies/171141/289", "user_cakedate": "2023-01-26", "user_birthdate": "1904-11-29", "reactions": [{"id": "heart", "type": "emoji", "count": 1}], "current_user_reaction": null, "reaction_users_count": 1, "current_user_used_main_reaction": false, "can_accept_answer": false, "can_unaccept_answer": false, "accepted_answer": false, "topic_accepted_answer": null}