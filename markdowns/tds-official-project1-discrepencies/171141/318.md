---
post_url: https://discourse.onlinedegree.iitm.ac.in/t/tds-official-project1-discrepencies/171141/318
reply_to_post_number: 303
---
[@Arunvembu](/u/arunvembu) [@22f2000008](/u/22f2000008) [@23f1000879](/u/23f1000879) [@22f3003201](/u/22f3003201) [@23f2000926](/u/23f2000926) [@22f3001702](/u/22f3001702) [@Santoshsharma](/u/santoshsharma) [@kartikay1](/u/kartikay1) [@Kasif](/u/kasif)

Check first by following the instructions show here:


[Tds-official-Project1-discrepencies](https://discourse.onlinedegree.iitm.ac.in/t/tds-official-project1-discrepencies/171141/316) [Tools in Data Science](/c/courses/tds-kb/34)

> To replicate the test environment:
> git clone <your repo url>
> cd <your repo directory>
> docker build -t <your image name>
> Run new docker image using
> docker run -e AIPROXY\_TOKEN=$AIPROXY\_TOKEN -P 8000:8000 <your image name>
> Keep datagen.py and evaluate.py in same folder
> uv run evaluate.py --email=<any email> --token\_counter 1 --external\_port 8000
> This then re-produces the exact environment how your application was tested.
> You have to provide a token from your environment for testing.
> The…

Then post with your queries after testing as mentioned above.  
Also check the evaluation logs first to see why it failed. Address that question.  
Posting “it ran before submission” is insufficient evidence.  
The whole point of deployability is that it runs anywhere at anytime.  
That is what is being tested, not that it ran on your machine (unless you replicate the test environment exactly).

Kind regards