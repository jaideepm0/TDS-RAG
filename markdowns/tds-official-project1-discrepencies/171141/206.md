---
post_url: https://discourse.onlinedegree.iitm.ac.in/t/tds-official-project1-discrepencies/171141/206
reply_to_post_number: 192
---
[@Jivraj](/u/jivraj) [@Carlton](/u/carlton) I completely understand that changes to the Docker image after the deadline cannot be accepted.

***However, there are specific cases like mine where the Project 1 submission successfully passed the sanity checks on Feb 15 and received a decent score when the evaluation results were released on Mar 29.***

```markdown
## Image Description

The image contains a list of files and links related to a Docker evaluation process. Below is a structured breakdown of the content:

1. **Evaluation log file**: A link to a Google Drive document containing performance reports on individual tasks.
   - URL: [Evaluation Log File](https://drive.google.com/file/d/1GYe44D8gieDOlu9dCrKdsKwVAQ7j_C-N/view?usp=drivesdk)

2. **Docker log file**: A link to a Google Drive document that provides technical performance details of the Docker container.
   - URL: [Docker Log File](https://drive.google.com/file/d/1VTVeD-lwg3CFFPUYAcUqNzaGD7MlzyeC/view?usp=drivesdk)

3. **Server start log file**: Separate logs for ARM and x86 architecture (attachment needed).

4. **Evaluation script file**: Contains separate logs for ARM and x86 (attachment needed). This file includes tests run against the submission and scoring mechanisms.

5. **Data generation file**: An attachment that is essential for generating data for the tasks.

6. **Docker orchestration file**: An attachment for managing the Docker image from Docker Hub and launching the container instance, including environment variables and port mappings.

7. **Solution script**: An attached zip file that provides a solution to the project through prompt engineering.

### Additional Information
- An ID for the evaluated Docker image is provided: `11aa22fc1545`.
```

But here’s the catch:\*\* Since the problem statement for Project 1 and Project 2 is nearly the same, I took the opportunity to improve upon my Project 1 and use it as the foundation for my Project 2 submission, which I did by:\*

* Implementing a ReAct-based workflow planning & orchestration agent, inspired by the paper [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629).
* Implementing various tools for web-serping, web-scraping, read-eval-print-loops interpreters for quick calculations, etc.
* Enhancing Shell-use & Python-use by improving upon the existing code interpreter I had implemented for P1. This allowed the agent to dynamically generate and execute code without hardcoding anything.
* Adding useful API endpoints, including an **`/api/`** multipart/form endpoint, alongside the existing **`/read`** and **`/run`** endpoints from Project 1, plus a **`/clear`** endpoint to reset the agent’s conversation memory if the context window gets saturated.
* **Deploying the entire project on a paid GCP VM Instance with a static IP**, utilizing my own OpenAI API key while keeping OpenAI’s API as a fallback in case AIPROXY ever gave up.

All this hard work evolved my project into something far beyond a simple Tool-Calling Agent—it essentially became a ReAct Principles based Computer-Using Agent capable of executing complex, non-linear workflows entirely within a container. And I’m not exaggerating: You could ask it to perform something like **hyperparameter tuning for a Random Forest Classifier, offloading the results locally on a JSON file and displaying its contents**, and it would do that for you—without **ever** declining the request. I like to think of it as a **terminal version of** [OpenAI’s Computer-Using Agent](https://openai.com/index/computer-using-agent/).

---

Given all the effort, time, and money that went into this, it’s incredibly discouraging to see my project **naturally fail a sanity check (Docker image digest mismatch) (because of the aforesaid updates)** and not get evaluated as a result. This is not the kind of experience that encourages students to learn, experiment, and innovate.

To clarify, **all the updates mentioned above took place after March 29**, **after Project 1 had already been evaluated, and results had been handed out.** Furthermore, we were **never informed** that a reevaluation would take place on April 1. Had I known, I would have ensured that my original submission remained unchanged and considered creating a duplicate of my Docker image and implementing all the aforementioned enhancements on it.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

My only request is that if my **updated P1 submission cannot be evaluated** due to the changes made after March 29 (before the P1 reevaluation on April 1), I would really appreciate it if my **original P1 eval score could be reinstated** instead of receiving a **0**—since it was already evaluated and graded.

Would highly appreciate your prompt support in this regard [@carlton](/u/carlton) [@Jivraj](/u/jivraj)