---
post_url: https://discourse.onlinedegree.iitm.ac.in/t/tds-official-project1-discrepencies/171141/289
reply_to_post_number: 285
---
Thanks for the clarification regarding the evaluation, [@carlton](/u/carlton). It’s a relief to know that my original submission was successfully evaluated. Had I known that the stricter evaluation script would pull the image matching the digest from the time of submission (which had been overwritten by April 1), I would’ve used a separate tag to avoid the issue altogether.

Regarding your point on gold plating — I completely understand and have come to appreciate the importance of building to spec from personal experience, especially in production or client-facing settings where fixed targets, maintainability, and minimal scope creep are key. That said, with TDS projects, my goal was purely exploratory — to explore the boundaries of what’s possible **within the scope of the problem statement**.

What began as just another (pun intended) *tedious* assignment slowly evolved into a hobbyist research project on LLM agents. 

*(…caution: long post ahead )*

I noticed that **test cases in Project 1 and 2 were highly specific and often overlapping on Python & Shell use**. While it would’ve been easy to hardcode 50+ Python functions to pass these cases (which, frankly, many of us were doing), it is non-scalable at best. I quickly realized that stochastic parrots + hardcoded functions were a recipe for disaster, especially considering the **inherent randomness in LLM-generated payloads**. No two payloads are exactly alike — even minor differences, like an absolute vs relative file path, or some hidden edge case could cause a hardcoded solution to fail unpredictably. There’s also really no way to predict an edge case caused by an LLM.

Some might suggest using `temperature=0` to get more deterministic LLM behavior — and while true to an extent, it does little to encourage exploration, especially in tasks that require self-correction based on environmental feedback. Prompt engineering too wouldn’t be helpful here as 4o-mini isn’t all that great at 0-shot instruction following, especially when the system prompt is already saturated with 50+ fine-grained instructions. There’s only so much stuff it can pay attention to.

**Hardcoded tool agents also aren’t really “agents” in my view— they’re more like passive AI powered regex matchers**: merely mapping inputs to functions by inferring from context window. That puts all the burden of answering on the hardcoded functions, leaving the agent itself uninvolved in the solutioning process. If they break, the agent will never try to ‘fix’ them and keep calling them like a broken record.

At the core of it, it’s all about **how much flexibility vs rigidity** we give to the agent. Fully rigid solutions (e.g. hardcoding) overfit and break easily; fully flexible ones (e.g. pure LLM based) hallucinate or drift off-target. The sweet spot lies somewhere in between — The right solution would naturally balance the lesser of two evils in an ideal ratio.

Since most LLMs already excel at code generation and structured solutioning, the most effective strategy that I figured out for the agent was to,

* Reason about the task, understand intent,
* Reflect, whether this problem is solvable using available tools without human intervention and design structured workflows, in whatever order appropriate (i.e. *design* a DAG, where each node can be a python step or a shell step or something else)
* Execute those workflows (*walk* the DAG) observing the feedback at each step and reiterating if needed.
* Observe the final result, and repeat if needed.

Interestingly, a similar framework was suggested in [this ICLR 2022 paper](https://arxiv.org/abs/2210.03629). That was all the validation I needed to know I was stepping in the right direction.

To make environment interaction possible, the agent didn’t need dozens of narrow tools — just a small, well-defined set of **general-purpose tools**:

* A REPL executor (for quick calcs)
* A Python script runner
* A Shell executor

With just these, it could handle most tasks flexibly and naturally — avoiding overengineering while still enabling powerful behaviors. Potentially allowing for full fledged Computer-Use via shell and so much more.

As for the fact that it ended up being capable of things beyond the scope of Project 1 (like training & tuning ML models autonomously, reporting results etc.) — that was **emergent behavior**, not deliberate gold plating. It was a pleasant surprise even to me. I’ve yet to discover more of such interesting hidden use cases. While some might naturally call it scope creeping (and yes that is true, given that we had a deadline, and a play-pretend client-business relationship with the course team), I saw it as an opportunity for exploration and research. *Frankly, I AM personally very keen about researching stuff!*

I am actually very thankful to the TDS course team & [@s.anand](/u/s.anand) for devising such a thoughtful project that sparked some interesting ideas that I can tinker with. **Food for thought! Really!**

As for my next project, I now have a fair idea of what I’ll be experimenting with— modalities.

PS: I’m not claiming it’s perfect or production-ready, or it should score a perfect 22/20, but it aligned well with what I believe was the spirit of these projects: **thoughtful use of LLMs in agent design**. At this point, I’m less concerned about the marks, I’m actually enjoying the thought joyride. 

---

**TL;DR**  
My approach doesn’t rely on regex or hardcoded mappings. Instead, it passes user input directly to an LLM, which then plans and executes workflows using general tools inside a containerized environment. It also learns from feedback and iterates — much like a human. The fact that it can do more than just the minimum spec is a byproduct of that framework. I’ve only just wired the pieces together.

Kind regards