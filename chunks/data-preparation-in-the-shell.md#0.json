{
  "id": "data-preparation-in-the-shell.md#0",
  "post_urls": [
    "https://tds.s-anand.net/#/data-preparation-in-the-shell.md",
    "https://tds.s-anand.net/#/../data-preparation-in-the-shell.md"
  ],
  "content": "## Data Preparation in the Shell\n\n[! [Data preparation in the shell](https://i.ytimg.com/vi_webp/XEdy4WK70vU/sddefault.webp)](https://youtu.be/XEdy4WK70vU)\n\nYou'll learn how to use UNIX tools to process and clean data, covering:\n\n- `curl` (or `wget`) to fetch data from websites. - `gzip` (or `xz`) to compress and decompress files. - `wc` to count lines, words, and characters in text. - `head` and `tail` to get the start and end of files. - `cut` to extract specific columns from text. - `uniq` to de-duplicate lines. - `sort` to sort lines. - `grep` to filter lines containing specific text. - `sed` to search and replace text. - `awk` for more complex text processing. Here are the links used in the video:\n\n- [Data preparation in the shell - Notebook](https://colab.research.google.com/drive/1KSFkQDK0v__XWaAaHKeQuIAwYV0dkTe8)\n- [Data Science at the Command Line](https://jeroenjanssens.com/dsatcl/)"
}