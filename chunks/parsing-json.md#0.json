{
  "id": "parsing-json.md#0",
  "post_urls": [
    "https://tds.s-anand.net/#/parsing-json.md",
    "https://tds.s-anand.net/#/../parsing-json.md"
  ],
  "content": "## Parsing JSON\n\nJSON is everywhere—APIs, logs, configuration files—and its nested or large structure can challenge memory and processing. In this tutorial, we'll explore tools to flatten, stream, and query JSON data efficiently. For example, we'll often need to process a multi-gigabyte log file from a web service where each record is a JSON object. [! [JSON Parsing in Python](https://i.ytimg.com/vi/1lxrb_ezP-g/sddefault.jpg)](https://youtu.be/1lxrb_ezP-g)\n\nThis requires us to handle complex nested structures, large files that don't fit in memory, or extract specific fields. Here are the key tools and techniques for efficient JSON parsing:\n\n| Tool                                        | Extract from JSON...   | Why                                                               |\n| ------------------------------------------- | ---------------------- | ----------------------------------------------------------------- |\n| [jq](#command-line-json-processing-with-jq) | JSON in the shell      | Quick data exploration and pipeline processing                    |\n| [JMESPath](#jmespath-queries)               | JSON in Python         | Handle complex queries with a clean syntax                        |\n| [ijson](#streaming-with-ijson)              | JSON streams in Python | Parse streaming/large JSON files memory-efficiently               |\n| [Pandas](#pandas-json-columns)              | JSON columns in Python | Fast analysis of structured data                                  |\n| [SQL JSON](#sql-json-functions)             | JSON in databases      | Combine structured and semi-structured data                       |\n| [DuckDB](#duckdb-json-processing)           | JSON anywhere          | Fast analysis of JSON files / databases without loading to memory |\n\n**Examples:**\n\n- Use Pandas when you need to transform API responses into a DataFrame for further analysis. - Leverage ijson when dealing with huge JSON logs where memory is at a premium. - Apply jq for quick, iterative exploration directly in your terminal. Practice with these resources:\n\n- [JSONPath Online Evaluator](https://jsonpath.com/): Test JSON queries\n- [jq play](https://jqplay.org/): Interactive jq query testing\n- [DuckDB JSON Tutorial](https://duckdb.org/docs/data/json): Learn DuckDB JSON functions\n\n### Command-line JSON Processing with jq\n\n[jq](https://jqlang.org/) is a versatile command-line tool for slicing, filtering, and transforming JSON. It excels in quick data exploration and can be integrated into shell scripts for automated data pipelines. **Example:** Sifting through server logs in JSON Lines format to extract error messages or aggregate metrics without launching a full-scale ETL process. ```bash\n# Extract specific fields from JSONL\ncat data.jsonl | jq -c 'select(.type == \"user\") | {id, name}'\n\n# Transform JSON structure\ncat data.json | jq '.items[] | {name: .name, count: .details.count}'\n\n# Filter and aggregate\ncat events.jsonl | jq -s 'group_by(.category) | map({category: . [0].category, count: length})'\n```\n\n### JMESPath Queries\n\n[JMESPath](https://jmespath.org/) offers a declarative query language to extract and transform data from nested JSON structures without needing verbose code. It's a neat alternative when you want to quickly pull out specific values or filter collections based on conditions. **Example:** Extracting user emails or filtering out inactive records from a complex JSON payload received from a cloud service. ```python\nimport jmespath\n\n# Example queries\ndata = {\n    \"locations\": [\n        {\"name\": \"Seattle\", \"state\": \"WA\", \"info\": {\"population\": 737015}},\n        {\"name\": \"Portland\", \"state\": \"OR\", \"info\": {\"population\": 652503}}\n    ]\n}\n\n# Find all cities with population > 700000\ncities = jmespath.search(\"locations[?info.population > `700000`].name\", data)\n```\n\n### Streaming with ijson\n\nLoading huge JSON files all at once can quickly exhaust system memory. [ijson](https://ijson.readthedocs.io/en/latest/) lets you stream and process JSON incrementally. This method is ideal when your JSON file is too large or when you only need to work with part of the data. **Example:** Processing a continuous feed from an API that returns a large JSON array, such as sensor data or event logs, while filtering on the fly. ```python\nimport ijson\n\nasync def process_large_json(filepath: str) -> list:\n    \"\"\"Process a large JSON file without loading it entirely into memory.\"\"\" results = []\n\n    with open(filepath, 'rb') as file:\n        # Stream objects under the 'items' key\n        parser = ijson.items(file, 'items.item')\n        async for item in parser:\n            if item['value'] > 100:  # Process conditionally\n                results.append(item)\n\n    return results\n```\n\n### Pandas JSON Columns\n\n[Pandas](https://pandas.pydata.org/) makes it easy to work with tabular data that includes JSON strings. When you receive API data where one column holds nested JSON, flattening these structures lets you analyze and visualize the data using familiar DataFrame operations. **Example:** Flattening customer records stored as nested JSON in a CSV file to extract demographic details and spending patterns. ```python\nimport pandas as pd\n\n# Parse JSON strings in a column\ndf = pd.DataFrame({'json_col': ['{\"name\": \"Alice\", \"age\": 30}', '{\"name\": \"Bob\", \"age\": 25}']})\ndf['parsed'] = df['json_col'].apply(pd.json_normalize)\n\n# Normalize nested JSON columns\ndf = pd.read_csv('data.csv')\ndf_normalized = pd.json_normalize(\n    df['nested_json'].apply(json.loads),\n    record_path=['items'],        # List of nested objects to unpack\n    meta=['id', 'timestamp']      # Keep these columns from parent\n)\n```\n\n### SQL JSON Functions\n\n[SQL](https://en.wikipedia.org/wiki/SQL:2016) supports built-in JSON functions allow you to query and manipulate JSON stored within relational databases. These are implemented by most popular databases, including\n[SQLite](https://www.sqlite.org/json1.html),\n[PostgreSQL](https://www.postgresql.org/docs/current/functions-json.html), and\n[MySQL](https://dev.mysql.com/doc/refman/8.4/en/json-function-reference.html). This is especially handy when you have a hybrid data model, combining structured tables with semi-structured JSON columns. **Example:** An application that stores user settings or application logs as JSON in a SQLite database, enabling quick lookups and modifications without external JSON parsing libraries. ```sql\nSELECT\n    json_extract(data, '$.name') as name,\n    json_extract(data, '$.details.age') as age\nFROM users\nWHERE json_extract(data, '$.active') = true\n```\n\n### DuckDB JSON Processing\n\n[DuckDB](https://duckdb.org/) shines when analyzing JSON/JSONL files directly, making it a powerful tool for data analytics without the overhead of loading entire datasets into memory. Its SQL-like syntax simplifies exploratory analysis on nested data. **Example:** Performing ad-hoc analytics on streaming JSON logs from a web service, such as calculating average response times or aggregating user behavior metrics. ```sql\nSELECT\n    json_extract_string(data, '$.user.name') as name,\n    avg(json_extract_float(data, '$.metrics.value')) as avg_value\nFROM read_json_auto('data/*.jsonl')\nGROUP BY 1\nHAVING avg_value > 100\n```"
}