{
  "id": "vision-models.md#0",
  "post_urls": [
    "https://tds.s-anand.net/#/vision-models.md",
    "https://tds.s-anand.net/#/../vision-models.md"
  ],
  "content": "## Vision Models\n\n[! [LLM Vision Models](https://i.ytimg.com/vi_webp/FgT_Mk_bakQ/sddefault.webp)](https://youtu.be/FgT_Mk_bakQ)\n\nYou'll learn how to use LLMs to interpret images and extract useful information, covering:\n\n- **Setting Up Vision Models**: Integrate vision capabilities with LLMs using APIs like OpenAI's Chat Completion. - **Sending Image URLs for Analysis**: Pass URLs or base64-encoded images to LLMs for processing. - **Reading Image Responses**: Get detailed textual descriptions of images, from scenic landscapes to specific objects like cricketers or bank statements. - **Extracting Data from Images**: Convert extracted image data to various formats like Markdown tables or JSON arrays. - **Handling Model Hallucinations**: Address inaccuracies in extraction results, understanding how different prompts can affect output quality. - **Cost Management for Vision Models**: Adjust detail settings (e.g., \"detail: low\") to balance cost and output precision. Here are the links used in the video:\n\n- [Jupyter Notebook](https://colab.research.google.com/drive/1bK0b1XMrZWImtw01T1w9NGraDkiVi8mS)\n- [OpenAI Chat API Reference](https://platform.openai.com/docs/api-reference/chat/create)\n- [OpenAI Vision Guide](https://platform.openai.com/docs/guides/vision)\n- [Sample images used](https://drive.google.com/drive/folders/14MFc7XmGIUDU4-vbmF9305c1SSQrM-gR)\n\nHere is an example of how to analyze an image using the OpenAI API. ```bash\ncurl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"model\": \"gpt-4o-mini\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\"type\": \"text\", \"text\": \"What is in this image? \"},\n          {\n            \"type\": \"image_url\",\n            \"detail\": \"low\",\n            \"image_url\": {\"url\": \"https://upload.wikimedia.org/wikipedia/commons/3/34/Correlation_coefficient.png\"}\n          }\n        ]\n      }\n    ]\n  }'\n```\n\nLet's break down the request:\n\n- `curl https://api.openai.com/v1/chat/completions`: The API endpoint for text generation. - `-H \"Content-Type: application/json\"`: The content type of the request. - `-H \"Authorization: Bearer $OPENAI_API_KEY\"`: The API key for authentication. - `-d`: The request body. - `\"model\": \"gpt-4o-mini\"`: The model to use for text generation. - `\"messages\":`: The messages to send to the model. - `\"role\": \"user\"`: The role of the message. - `\"content\":`: The content of the message. - `{\"type\": \"text\", \"text\": \"What is in this image? \"}`: The text message. - `{\"type\": \"image_url\"}`: The image message. - `\"detail\": \"low\"`: The detail level of the image. `low` uses fewer tokens at lower detail. `high` uses more tokens for higher detail. - `\"image_url\": {\"url\": \"https://upload.wikimedia.org/wikipedia/commons/3/34/Correlation_coefficient.png\"}`: The URL of the image. You can send images in a [base64 encoded format](base64-image.md), too. For example:\n\n```bash\n# Download image and convert to base64 in one step\nIMAGE_BASE64=$(curl -s \"https://upload.wikimedia.org/wikipedia/commons/3/34/Correlation_coefficient.png\" | base64 -w 0)\n\n# Send to OpenAI API\ncurl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d @- << EOF\n{\n  \"model\": \"gpt-4o-mini\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\"type\": \"text\", \"text\": \"What is in this image? \"},\n        {\n          \"type\": \"image_url\",\n          \"image_url\": { \"url\": \"data:image/png;base64,$IMAGE_BASE64\" }\n        }\n      ]\n    }\n  ]\n}\nEOF\n```"
}